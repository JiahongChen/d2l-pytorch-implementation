{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "L12/4 AlexNet",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuMnA0x_6BCX",
        "colab_type": "text"
      },
      "source": [
        "# **AlexNet**\n",
        "\n",
        "AlexNet improves the LeNet using bigger and deeper networks. It further improves the performance with ReLU, Dropout, and MaxPolling.\n",
        "\n",
        "In this notebook, the 28$\\times$28 Fasion-MNIST is scale up to 224$\\times$224, with 10 class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ks90tCig4I8F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "from torchvision import transforms\n",
        "import torch.optim as optim\n",
        "import time\n",
        "\n",
        "batch_size = 128\n",
        "num_epochs = 5\n",
        "# Note not to flip two transform types, otherwise data type would be wrong.\n",
        "transform = transforms.Compose([transforms.Resize(224),\n",
        "                                transforms.ToTensor(),\n",
        "                              ]) \n",
        "\n",
        "mnist_trainset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "mnist_testset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aw4xxoxm6wny",
        "colab_type": "text"
      },
      "source": [
        "AlexNet uses extra three sucessive convolutional layers to learn the features, and uses ReLU, dropout in the classification stage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWIYexcS4mVg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 96, kernel_size=11, stride=4, padding=1)\n",
        "        self.conv2 = nn.Conv2d(96, 256, kernel_size=5, padding=2)\n",
        "        self.conv3 = nn.Conv2d(256, 384, kernel_size=3, padding=1)\n",
        "        self.conv4 = nn.Conv2d(384, 384, kernel_size=3, padding=1)\n",
        "        self.conv5 = nn.Conv2d(384, 256, kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(6400, 4096)\n",
        "        self.fc2 = nn.Linear(4096, 4096)\n",
        "        self.fc3 = nn.Linear(4096, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.maxpool(self.relu(self.conv1(x)))\n",
        "        x = self.maxpool(self.relu(self.conv2(x)))\n",
        "        # Then, use three sucessive convolutional layers and a smaller covolution window\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.conv5(x)\n",
        "        # dimensional reduction using maxpooling\n",
        "        x = self.maxpool(x)\n",
        "        # Flatten convotional layers output for classifier.\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        # Classifier using three fully connected layers.\n",
        "        x = self.dropout(self.relu(self.fc1(x)))\n",
        "        x = self.dropout(self.relu(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        m.bias.data.normal_(0.0, 0.01)\n",
        "    elif classname.find('Linear') != -1:\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        m.bias.data.normal_(0.0, 0.01)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.normal_(1.0, 0.01)\n",
        "        m.bias.data.fill_(0)\n",
        "\n",
        "def evaluate_accuracy(data_iter, net):\n",
        "    \"\"\"Evaluate accuracy of a model on the given data set.\"\"\"\n",
        "    acc_sum,n = 0,0\n",
        "    for (imgs, labels) in data_iter:\n",
        "        # send data to the GPU if cuda is availabel\n",
        "        if torch.cuda.is_available():\n",
        "            imgs = imgs.cuda()\n",
        "            labels = labels.cuda()\n",
        "        net.eval()\n",
        "        with torch.no_grad():\n",
        "            labels = labels.long()\n",
        "            acc_sum += torch.sum((torch.argmax(net(imgs), dim=1) == labels)).float()\n",
        "            n += labels.shape[0]\n",
        "    return acc_sum.item()/n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymsSlNOU8ret",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "039d3e71-616b-4126-e270-01d3c3f0153f"
      },
      "source": [
        "# Loading training set and test set using DataLoader.\n",
        "train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=batch_size,\n",
        "    shuffle=True, num_workers=0)\n",
        "test_loader = torch.utils.data.DataLoader(mnist_testset, batch_size=batch_size,\n",
        "    shuffle=True, num_workers=0)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print('Training using GPU.')\n",
        "    net = LeNet().cuda()\n",
        "else:\n",
        "    print('Training using CPU.')\n",
        "    net = LeNet()\n",
        "\n",
        "#Initialize network parameters.\n",
        "net.apply(weights_init)\n",
        "\n",
        "#Loss function\n",
        "if torch.cuda.is_available():\n",
        "    loss = nn.CrossEntropyLoss().cuda()\n",
        "else:\n",
        "    loss = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train using SGD optimizer \n",
        "lr= 0.01 # Compare to LeNet, the learning rate is much smaller due to much larget images\n",
        "opt_n = optim.SGD(net.parameters(), lr=lr)\n",
        "\n",
        "# Training stage\n",
        "for epoch in range(1, num_epochs+1):\n",
        "    train_loader_iter = iter(train_loader)\n",
        "    train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
        "    \n",
        "    for (imgs, labels) in train_loader_iter:\n",
        "        net.train()\n",
        "        opt_n.zero_grad()\n",
        "        if torch.cuda.is_available():\n",
        "            imgs = imgs.cuda()\n",
        "            labels = labels.cuda()\n",
        "        # Label prediction from LeNet\n",
        "        y_hat = net(imgs)\n",
        "        l = loss(y_hat, labels)\n",
        "        # Backprobagation\n",
        "        l.backward()\n",
        "        opt_n.step()\n",
        "\n",
        "        # Calculate tarining error\n",
        "        with torch.no_grad():\n",
        "            labels = labels.long()\n",
        "            train_l_sum += l.item()\n",
        "            train_acc_sum += (torch.sum(torch.argmax(y_hat, dim=1) == labels)).float().item()\n",
        "            n += labels.shape[0]\n",
        "    # calculate testing error every epoch.\n",
        "    test_acc = evaluate_accuracy(iter(test_loader), net)\n",
        "    print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\n",
        "          % (epoch, train_l_sum/n, train_acc_sum/n, test_acc,\n",
        "            time.time() - start))\n",
        "\n",
        "\n",
        "            \n",
        "\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training using GPU.\n",
            "epoch 1, loss 0.0081, train acc 0.616, test acc 0.777, time 64.2 sec\n",
            "epoch 2, loss 0.0044, train acc 0.791, test acc 0.824, time 64.2 sec\n",
            "epoch 3, loss 0.0037, train acc 0.826, test acc 0.848, time 64.3 sec\n",
            "epoch 4, loss 0.0033, train acc 0.845, test acc 0.861, time 64.4 sec\n",
            "epoch 5, loss 0.0030, train acc 0.859, test acc 0.870, time 64.1 sec\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}