{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "L12/6 VGG",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuMnA0x_6BCX",
        "colab_type": "text"
      },
      "source": [
        "# **VGG**\n",
        "\n",
        "Visual Geometry Group (VGG) is basicly a bigger and deeper AlexNet with repeated VGG blocks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ks90tCig4I8F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "from torchvision import transforms\n",
        "import torch.optim as optim\n",
        "import time\n",
        "\n",
        "batch_size = 128\n",
        "num_epochs = 5\n",
        "# Note not to flip two transform types, otherwise data type would be wrong.\n",
        "transform = transforms.Compose([transforms.Resize(224),\n",
        "                                transforms.ToTensor(),\n",
        "                              ]) \n",
        "\n",
        "mnist_trainset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "mnist_testset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aw4xxoxm6wny",
        "colab_type": "text"
      },
      "source": [
        "# **Implementation for VGG11**\n",
        "Key innovation in VGG is that the group layers into blocks which then turns into parameterizable repeated blocks that used for computer vision."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWIYexcS4mVg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vgg_block(num_convs, num_in_channels, num_out_channels):\n",
        "    ''' Basic VGG block.\n",
        "        Inputs: \n",
        "                num_convs: number of convolutional layers in this VGG block\n",
        "                num_in_channels: number of input channels of the VGG block\n",
        "                num_out_channels: number of output channels of the VGG block\n",
        "        Output: the VGG block at the given shape\n",
        "    '''\n",
        "    blk = []\n",
        "    for _ in range(num_convs):\n",
        "        blk.append(nn.Conv2d(num_in_channels, num_out_channels, kernel_size=3, padding=1))\n",
        "        blk.append(nn.ReLU())\n",
        "        num_in_channels = num_out_channels\n",
        "    blk.append(nn.MaxPool2d(kernel_size=2,stride=2))\n",
        "    return nn.Sequential(*blk)\n",
        "\n",
        "\n",
        "class flatten(nn.Module):\n",
        "    ''' Flatten convotional layers output for classifier.'''\n",
        "    def forward(self, x):\n",
        "        return x.view(x.shape[0], -1)\n",
        "\n",
        "# Establish VGG using vgg_block\n",
        "def VGG(conv_arch):\n",
        "    net = []\n",
        "    num_in_channels = 1\n",
        "    for (num_convs, num_out_channels) in conv_arch:\n",
        "        net.append(vgg_block(num_convs, num_in_channels, num_out_channels))\n",
        "        # Only the first input channel in a VGG block is different from the \n",
        "        # number of output channel in the same block. The number of first input\n",
        "        # channels is the number of output channels in the previous VGG block\n",
        "        num_in_channels = num_out_channels\n",
        "\n",
        "    vgg_net=nn.Sequential(\n",
        "                      *net,\n",
        "                      flatten(),\n",
        "                      # Classifier using three fully connected layers,\n",
        "                      # with ReLU activation function and dropout rate at 0.5.\n",
        "                      nn.Linear(512*7*7, 4096),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Dropout(0.5),\n",
        "                      nn.Linear(4096, 4096),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Dropout(0.5),\n",
        "                      nn.Linear(4096, 10)\n",
        "                     )\n",
        "    return vgg_net\n",
        "\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        m.bias.data.normal_(0.0, 0.01)\n",
        "    elif classname.find('Linear') != -1:\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        m.bias.data.normal_(0.0, 0.01)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.normal_(1.0, 0.01)\n",
        "        m.bias.data.fill_(0)\n",
        "\n",
        "def evaluate_accuracy(data_iter, net):\n",
        "    \"\"\"Evaluate accuracy of a model on the given data set.\"\"\"\n",
        "    acc_sum,n = 0,0\n",
        "    for (imgs, labels) in data_iter:\n",
        "        # send data to the GPU if cuda is availabel\n",
        "        if torch.cuda.is_available():\n",
        "            imgs = imgs.cuda()\n",
        "            labels = labels.cuda()\n",
        "        net.eval()\n",
        "        with torch.no_grad():\n",
        "            labels = labels.long()\n",
        "            acc_sum += torch.sum((torch.argmax(net(imgs), dim=1) == labels)).float()\n",
        "            n += labels.shape[0]\n",
        "    return acc_sum.item()/n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5MfLkOn4dtm",
        "colab_type": "text"
      },
      "source": [
        "# **Train with Implemented VGG**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymsSlNOU8ret",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "86448dd8-34a3-4b40-dbf8-510ab38055c7"
      },
      "source": [
        "# Loading training set and test set using DataLoader.\n",
        "train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=batch_size,\n",
        "    shuffle=True, num_workers=0)\n",
        "test_loader = torch.utils.data.DataLoader(mnist_testset, batch_size=batch_size,\n",
        "    shuffle=True, num_workers=0)\n",
        "\n",
        "# Architecure of VGG-11\n",
        "conv_arch = ((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print('Training using GPU.')\n",
        "    net = VGG(conv_arch).cuda()\n",
        "else:\n",
        "    print('Training using CPU.')\n",
        "    net = VGG(conv_arch)\n",
        "\n",
        "#Initialize network parameters.\n",
        "net.apply(weights_init)\n",
        "\n",
        "#Loss function\n",
        "if torch.cuda.is_available():\n",
        "    loss = nn.CrossEntropyLoss().cuda()\n",
        "else:\n",
        "    loss = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train using SGD optimizer \n",
        "lr= 0.05 # Compare to LeNet, the learning rate is much smaller due to much larget images\n",
        "opt_n = optim.SGD(net.parameters(), lr=lr)\n",
        "\n",
        "# Training stage\n",
        "from tqdm import tqdm\n",
        "for epoch in range(1, num_epochs+1):\n",
        "    train_loader_iter = iter(train_loader)\n",
        "    train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
        "    \n",
        "    for _, (imgs, labels) in tqdm(enumerate(train_loader_iter)):\n",
        "        net.train()\n",
        "        opt_n.zero_grad()\n",
        "        if torch.cuda.is_available():\n",
        "            imgs = imgs.cuda()\n",
        "            labels = labels.cuda()\n",
        "        # Label prediction from LeNet\n",
        "        y_hat = net(imgs)\n",
        "        l = loss(y_hat, labels)\n",
        "        # Backprobagation\n",
        "        l.backward()\n",
        "        opt_n.step()\n",
        "\n",
        "        # Calculate tarining error\n",
        "        with torch.no_grad():\n",
        "            labels = labels.long()\n",
        "            train_l_sum += l.item()\n",
        "            train_acc_sum += (torch.sum(torch.argmax(y_hat, dim=1) == labels)).float().item()\n",
        "            n += labels.shape[0]\n",
        "    # calculate testing error every epoch.\n",
        "    test_acc = evaluate_accuracy(iter(test_loader), net)\n",
        "    print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\n",
        "          % (epoch, train_l_sum/n, train_acc_sum/n, test_acc,\n",
        "            time.time() - start))\n",
        "\n",
        "\n",
        "            \n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training using GPU.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "469it [06:55,  1.13it/s]\n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 1, loss 0.0126, train acc 0.399, test acc 0.783, time 437.1 sec\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "469it [06:57,  1.12it/s]\n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 2, loss 0.0037, train acc 0.825, test acc 0.857, time 439.6 sec\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "469it [06:58,  1.12it/s]\n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 3, loss 0.0027, train acc 0.871, test acc 0.881, time 439.7 sec\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "469it [06:57,  1.12it/s]\n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 4, loss 0.0023, train acc 0.889, test acc 0.890, time 439.5 sec\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "469it [06:57,  1.12it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 5, loss 0.0021, train acc 0.903, test acc 0.896, time 439.6 sec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHI6uoRd3FE2",
        "colab_type": "text"
      },
      "source": [
        "# **Train with Pytorch VGG model**\n",
        "In Pytorch, you can also call the implemented and pretrained VGG models. In the following notebook, we call VGG11 to compare with the self-implemented model. \n",
        "\n",
        "We can also call VGG13, VGG16, VGG19 (and the versions with batrh normalization)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptGSf2RZ3ayq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sample code\n",
        "import torchvision.models as models\n",
        "pretrained = False\n",
        "net = models.vgg11(pretrained=pretrained).cuda()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
