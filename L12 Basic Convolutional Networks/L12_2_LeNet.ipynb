{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "L12/2 LeNet",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuMnA0x_6BCX",
        "colab_type": "text"
      },
      "source": [
        "LeNet is the first convolutional neural network. This note book reproduces the LeNet using Fashion-MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ks90tCig4I8F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "from torchvision import transforms\n",
        "import torch.optim as optim\n",
        "import time\n",
        "\n",
        "batch_size = 256\n",
        "num_epochs = 20\n",
        "transform = transforms.Compose([transforms.ToTensor()]) \n",
        "\n",
        "mnist_trainset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "mnist_testset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aw4xxoxm6wny",
        "colab_type": "text"
      },
      "source": [
        "LeNet uses two convolutional layers to extract the input features, and three fully connected layers are used as classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWIYexcS4mVg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, padding=2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
        "        self.fc1 = nn.Linear(16*5*5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.avgpool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Feature extraction using to convolutional layers.\n",
        "        x = self.avgpool(self.sigmoid(self.conv1(x)))\n",
        "        x = self.avgpool(self.sigmoid(self.conv2(x)))\n",
        "        #reshape the tensor to 1-d to fit the FC layer input\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        # Classifier using three fully connected layers.\n",
        "        x = self.sigmoid(self.fc1(x))\n",
        "        x = self.sigmoid(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        m.bias.data.normal_(0.0, 0.01)\n",
        "    elif classname.find('Linear') != -1:\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        m.bias.data.normal_(0.0, 0.01)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.normal_(1.0, 0.01)\n",
        "        m.bias.data.fill_(0)\n",
        "\n",
        "def evaluate_accuracy(data_iter, net):\n",
        "    \"\"\"Evaluate accuracy of a model on the given data set.\"\"\"\n",
        "    acc_sum,n = 0,0\n",
        "    for (imgs, labels) in data_iter:\n",
        "        # send data to the GPU if cuda is availabel\n",
        "        if torch.cuda.is_available():\n",
        "            imgs = imgs.cuda()\n",
        "            labels = labels.cuda()\n",
        "        net.eval()\n",
        "        with torch.no_grad():\n",
        "            labels = labels.long()\n",
        "            acc_sum += torch.sum((torch.argmax(net(imgs), dim=1) == labels)).float()\n",
        "            n += labels.shape[0]\n",
        "    return acc_sum.item()/n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymsSlNOU8ret",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "5b262e51-1975-4747-c5b2-8453152ac768"
      },
      "source": [
        "# Loading training set and test set using DataLoader.\n",
        "train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=batch_size,\n",
        "    shuffle=True, num_workers=0)\n",
        "test_loader = torch.utils.data.DataLoader(mnist_testset, batch_size=batch_size,\n",
        "    shuffle=True, num_workers=0)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print('Training using GPU.')\n",
        "    net = LeNet().cuda()\n",
        "else:\n",
        "    print('Training using CPU.')\n",
        "    net = LeNet()\n",
        "\n",
        "#Initialize network parameters.\n",
        "net.apply(weights_init)\n",
        "\n",
        "#Loss function\n",
        "if torch.cuda.is_available():\n",
        "    loss = nn.CrossEntropyLoss().cuda()\n",
        "else:\n",
        "    loss = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train using SGD optimizer \n",
        "lr= 0.3 # This learning rate was not fine-tuned.\n",
        "opt_n = optim.SGD(net.parameters(), lr=lr)\n",
        "\n",
        "# Training stage\n",
        "for epoch in range(1, num_epochs+1):\n",
        "    train_loader_iter = iter(train_loader)\n",
        "    train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
        "    \n",
        "    for (imgs, labels) in train_loader_iter:\n",
        "        net.train()\n",
        "        opt_n.zero_grad()\n",
        "        if torch.cuda.is_available():\n",
        "            imgs = imgs.cuda()\n",
        "            labels = labels.cuda()\n",
        "        # Label prediction from LeNet\n",
        "        y_hat = net(imgs)\n",
        "        l = loss(y_hat, labels)\n",
        "        # Backprobagation\n",
        "        l.backward()\n",
        "        opt_n.step()\n",
        "\n",
        "        # Calculate tarining error\n",
        "        with torch.no_grad():\n",
        "            labels = labels.long()\n",
        "            train_l_sum += l.item()\n",
        "            train_acc_sum += (torch.sum(torch.argmax(y_hat, dim=1) == labels)).float().item()\n",
        "            n += labels.shape[0]\n",
        "    # calculate testing error every epoch.\n",
        "    test_acc = evaluate_accuracy(iter(test_loader), net)\n",
        "    print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\n",
        "          % (epoch, train_l_sum/n, train_acc_sum/n, test_acc,\n",
        "            time.time() - start))\n",
        "\n",
        "\n",
        "            \n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training using GPU.\n",
            "epoch 1, loss 0.0091, train acc 0.099, test acc 0.100, time 4.4 sec\n",
            "epoch 2, loss 0.0090, train acc 0.110, test acc 0.100, time 4.4 sec\n",
            "epoch 3, loss 0.0066, train acc 0.410, test acc 0.568, time 4.4 sec\n",
            "epoch 4, loss 0.0040, train acc 0.608, test acc 0.608, time 4.4 sec\n",
            "epoch 5, loss 0.0034, train acc 0.673, test acc 0.701, time 4.5 sec\n",
            "epoch 6, loss 0.0031, train acc 0.698, test acc 0.676, time 4.5 sec\n",
            "epoch 7, loss 0.0029, train acc 0.720, test acc 0.692, time 4.5 sec\n",
            "epoch 8, loss 0.0027, train acc 0.735, test acc 0.714, time 4.5 sec\n",
            "epoch 9, loss 0.0026, train acc 0.744, test acc 0.738, time 4.5 sec\n",
            "epoch 10, loss 0.0025, train acc 0.751, test acc 0.683, time 4.4 sec\n",
            "epoch 11, loss 0.0024, train acc 0.762, test acc 0.762, time 4.5 sec\n",
            "epoch 12, loss 0.0023, train acc 0.770, test acc 0.754, time 4.5 sec\n",
            "epoch 13, loss 0.0023, train acc 0.775, test acc 0.766, time 4.5 sec\n",
            "epoch 14, loss 0.0022, train acc 0.784, test acc 0.764, time 4.6 sec\n",
            "epoch 15, loss 0.0021, train acc 0.789, test acc 0.773, time 4.5 sec\n",
            "epoch 16, loss 0.0021, train acc 0.796, test acc 0.756, time 4.5 sec\n",
            "epoch 17, loss 0.0020, train acc 0.803, test acc 0.761, time 4.4 sec\n",
            "epoch 18, loss 0.0020, train acc 0.807, test acc 0.788, time 4.5 sec\n",
            "epoch 19, loss 0.0019, train acc 0.814, test acc 0.774, time 4.4 sec\n",
            "epoch 20, loss 0.0019, train acc 0.817, test acc 0.815, time 4.5 sec\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
