{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "L5/8 Softmax Regression in PyTorch",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rhux1pqBXoRn",
        "colab_type": "text"
      },
      "source": [
        "# **Softmax Regression from Scratch**\n",
        "We use the Fashion-MNIST data set with batch size 256."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qznhC1TFXkkz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "from torchvision import transforms\n",
        "import torch.optim as optim\n",
        "import time\n",
        "\n",
        "batch_size = 256\n",
        "transform = transforms.Compose([transforms.ToTensor(),]) \n",
        "\n",
        "mnist_train = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "mnist_test = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size,\n",
        "    shuffle=True, num_workers=0)\n",
        "test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size,\n",
        "    shuffle=True, num_workers=0)\n",
        "\n",
        "num_epochs, lr = 5, 0.1"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsGbniA7YWfZ",
        "colab_type": "text"
      },
      "source": [
        "# **The Softmax**\n",
        "We can now define the softmax function. For that we rst exponentiate each term using $\\exp$ and then sum each row to get the normalization constant. Last we divide each row by its normalization constant and return the result.\n",
        "\n",
        "$\\textrm{softmax}(\\mathbf{X})_{ij}=\\frac{\\exp(X_{ij})}{\\sum_k\\exp (X_{ik})}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOSdwXmkYVV5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(X):\n",
        "    X_exp = X.exp()\n",
        "    partition = X_exp.sum(axis=1, keepdims=True)\n",
        "    return X_exp / partition # The broadcast mechanism is applied here."
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGrhZhHgYyny",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "aebc77d8-330c-4edc-8dfa-94d3e27ef213"
      },
      "source": [
        "X = torch.normal(0, 1, size=(2, 5))\n",
        "X_prob = softmax(X)\n",
        "print(X_prob, X_prob.sum(axis=1))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.0943, 0.3198, 0.2472, 0.1737, 0.1650],\n",
            "        [0.2237, 0.4788, 0.0664, 0.0415, 0.1896]]) tensor([1.0000, 1.0000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayPSNl5QZFxL",
        "colab_type": "text"
      },
      "source": [
        "# **The Model and Parameters Initialization**\n",
        "Since each example is an image with $28\\times 28$ pixels we can store it as a 784 dimensional\n",
        "vector. Moreover, since we have 10 categories, the single layer network has an output\n",
        "dimension of 10."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpUITfGEZGSi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "e1bba8af-2731-41df-8a60-8990e3e5ca73"
      },
      "source": [
        "class Flatten(torch.nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x.view(-1,784)\n",
        "\n",
        "net = nn.Sequential(Flatten(), nn.Linear(784, 10))\n",
        "\n",
        "def init_weights(m):\n",
        "    if type(m) == nn.Linear:\n",
        "        # Initialize weight parameter by a normal distribition \n",
        "        # with a mean of 0 and standard deviation of 0.01.\n",
        "        nn.init.normal_(m.weight.data, std=0.01)\n",
        "        # The bias parameter is initialized to zero by default.\n",
        "        m.bias.data.fill_(0.0)\n",
        "\n",
        "net.apply(init_weights)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Flatten()\n",
              "  (1): Linear(in_features=784, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEm2MP5mZMod",
        "colab_type": "text"
      },
      "source": [
        "# **The Loss Function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggaOr0y_ZM_s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss = nn.CrossEntropyLoss()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WigFfRT_cA6v",
        "colab_type": "text"
      },
      "source": [
        "# **Optimization Algorithm**\n",
        "We use SGD with a learning rate of 0.1, just as in linear regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWjW2Uobb-gZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainer = optim.SGD(net.parameters(), lr=lr)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTPqh3HCZ5Rn",
        "colab_type": "text"
      },
      "source": [
        "# **Classication Accuracy**\n",
        "Given a class of predicted probability distributions *y_hat* , we use the one with the highest\n",
        "predicted probability as the output category."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5owdArDKZnzo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_accuracy(data_iter, net):\n",
        "    \"\"\"Evaluate accuracy of a model on the given data set.\"\"\"\n",
        "    acc_sum,n = 0,0\n",
        "    for (imgs, labels) in data_iter:\n",
        "        net.eval()\n",
        "        with torch.no_grad():\n",
        "            labels = labels.float()\n",
        "            acc_sum += torch.sum((torch.argmax(net(imgs), dim=1) == labels)).float()\n",
        "            n += labels.shape[0]\n",
        "    return acc_sum.item()/n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwSYC3_xad11",
        "colab_type": "text"
      },
      "source": [
        "Because we initialized the net model with random weights, the accuracy of this model should be close to random guessing, i.e. 0.1 for 10 classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGQPZ2TvaMHB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3bf359f1-b966-48be-94f0-edc8198aa3a6"
      },
      "source": [
        "evaluate_accuracy(test_loader, net)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.056"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9GwRDjhapFC",
        "colab_type": "text"
      },
      "source": [
        "# **Model Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMlXc0Xbaoe3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "af5650bf-b015-4d91-f397-849a6e65ecf0"
      },
      "source": [
        "for epoch in range(1, num_epochs+1):\n",
        "    train_loader_iter = iter(train_loader)\n",
        "    train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
        "    \n",
        "    for (imgs, labels) in train_loader_iter:\n",
        "        net.train()\n",
        "        y_hat = net(imgs)\n",
        "        l = loss(y_hat, labels)\n",
        "        # Backprobagation\n",
        "        l.backward()\n",
        "        trainer.step()\n",
        "\n",
        "        # Calculate tarining error\n",
        "        with torch.no_grad():\n",
        "            labels = labels.float()\n",
        "            train_l_sum += l.item()\n",
        "            train_acc_sum += (torch.sum(torch.argmax(y_hat, dim=1) == labels)).float().item()\n",
        "            n += labels.shape[0]\n",
        "    # calculate testing error every epoch.\n",
        "    test_acc = evaluate_accuracy(iter(test_loader), net)\n",
        "    print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\n",
        "          % (epoch, train_l_sum/n, train_acc_sum/n, test_acc,\n",
        "            time.time() - start))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1, loss 0.0097, train acc 0.768, test acc 0.814, time 5.5 sec\n",
            "epoch 2, loss 0.0184, train acc 0.804, test acc 0.799, time 5.8 sec\n",
            "epoch 3, loss 0.0291, train acc 0.804, test acc 0.798, time 8.8 sec\n",
            "epoch 4, loss 0.0365, train acc 0.814, test acc 0.804, time 5.5 sec\n",
            "epoch 5, loss 0.0366, train acc 0.814, test acc 0.794, time 5.6 sec\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}