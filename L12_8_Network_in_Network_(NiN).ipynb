{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "L12/8 Network in Network (NiN)",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuMnA0x_6BCX",
        "colab_type": "text"
      },
      "source": [
        "# **NiN**\n",
        "\n",
        "Network in Network (NiN) Get rid of the fully connected layers, it replaced AlexNet's densse layers with NiN blocks, and act like a global average pooling layer to comnine outputs. Although they may not perform as well as VGG, they were a key thing to go to inception or ResNet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ks90tCig4I8F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "from torchvision import transforms\n",
        "import torch.optim as optim\n",
        "import time\n",
        "\n",
        "batch_size = 128\n",
        "num_epochs = 5\n",
        "# Note not to flip two transform types, otherwise data type would be wrong.\n",
        "transform = transforms.Compose([transforms.Resize(224),\n",
        "                                transforms.ToTensor(),\n",
        "                              ]) \n",
        "\n",
        "mnist_trainset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "mnist_testset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aw4xxoxm6wny",
        "colab_type": "text"
      },
      "source": [
        "# **Implementation for NiN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWIYexcS4mVg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def nin_block(num_in_channels, num_out_channels, kernel_size, strides, padding):\n",
        "    ''' NiN block.\n",
        "        Inputs: \n",
        "                num_in_channels: number of input channels of the NiN block\n",
        "                num_out_channels: number of output channels of the NiN block\n",
        "        Output: the NiN block at the given shape\n",
        "    '''\n",
        "    blk = nn.Sequential(\n",
        "            nn.Conv2d(num_in_channels, num_out_channels, kernel_size, strides, padding),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(num_out_channels, num_out_channels, kernel_size=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(num_out_channels, num_out_channels, kernel_size=1),\n",
        "            nn.ReLU())\n",
        "    return blk\n",
        "\n",
        "\n",
        "class flatten(nn.Module):\n",
        "    ''' Flatten convotional layers output for classifier.'''\n",
        "    def forward(self, x):\n",
        "        return x.view(x.shape[0], -1)\n",
        "\n",
        "# Establish NiN using nin_block\n",
        "def nin():\n",
        "    nin_1 = nin_block(num_in_channels=1,num_out_channels=96, kernel_size=11, strides=4, padding=0)\n",
        "    nin_2 = nin_block(num_in_channels=96,num_out_channels=256, kernel_size=5, strides=1, padding=2)\n",
        "    nin_3 = nin_block(num_in_channels=256,num_out_channels=384, kernel_size=3, strides=1, padding=1)\n",
        "    nin_4 = nin_block(num_in_channels=384,num_out_channels=10, kernel_size=3, strides=1, padding=1)\n",
        "    nin_net=nn.Sequential(\n",
        "                      *nin_1,\n",
        "                      nn.MaxPool2d(3,stride=2),\n",
        "                      *nin_2,\n",
        "                      nn.MaxPool2d(3,stride=2),\n",
        "                      *nin_3,\n",
        "                      nn.MaxPool2d(3,stride=2),\n",
        "                      nn.Dropout2d(0.5),\n",
        "                      # Get 10 label classes\n",
        "                      *nin_4,\n",
        "                      # Global Average Pooling\n",
        "                      nn.AdaptiveMaxPool2d((1,1)),\n",
        "                      # Transform the four-dimensional output into 2D output\n",
        "                      # flatten()\n",
        "                      torch.nn.Flatten()\n",
        "                     )\n",
        "    return nin_net\n",
        "\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        m.bias.data.normal_(0.0, 0.01)\n",
        "    elif classname.find('Linear') != -1:\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        m.bias.data.normal_(0.0, 0.01)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.normal_(1.0, 0.01)\n",
        "        m.bias.data.fill_(0)\n",
        "\n",
        "def evaluate_accuracy(data_iter, net):\n",
        "    \"\"\"Evaluate accuracy of a model on the given data set.\"\"\"\n",
        "    acc_sum,n = 0,0\n",
        "    for (imgs, labels) in data_iter:\n",
        "        # send data to the GPU if cuda is availabel\n",
        "        if torch.cuda.is_available():\n",
        "            imgs = imgs.cuda()\n",
        "            labels = labels.cuda()\n",
        "        net.eval()\n",
        "        with torch.no_grad():\n",
        "            labels = labels.long()\n",
        "            acc_sum += torch.sum((torch.argmax(net(imgs), dim=1) == labels)).float()\n",
        "            n += labels.shape[0]\n",
        "    return acc_sum.item()/n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTGSx7OlKSf7",
        "colab_type": "text"
      },
      "source": [
        "NiN required significantly less memories to be computed, which is about 3.16 GB. In comparison, the VGG11 with same dataset and batch size consummes more than 14 GB of memory.\n",
        "\n",
        "The computation time for NiN also computed much faster than VGG (~80 sec/epoch vs ~400 sec/epoch)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymsSlNOU8ret",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "431c2a34-7585-479a-95e3-968bf6242f90"
      },
      "source": [
        "# Loading training set and test set using DataLoader.\n",
        "train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=batch_size,\n",
        "    shuffle=True, num_workers=0)\n",
        "test_loader = torch.utils.data.DataLoader(mnist_testset, batch_size=batch_size,\n",
        "    shuffle=True, num_workers=0)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print('Training using GPU.')\n",
        "    net = nin().cuda()\n",
        "else:\n",
        "    print('Training using CPU.')\n",
        "    net = nin()\n",
        "\n",
        "#Initialize network parameters.\n",
        "net.apply(weights_init)\n",
        "\n",
        "#Loss function\n",
        "if torch.cuda.is_available():\n",
        "    loss = nn.CrossEntropyLoss().cuda()\n",
        "else:\n",
        "    loss = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train using SGD optimizer \n",
        "lr= 0.1 # Compare to LeNet, the learning rate is much smaller due to much larget images\n",
        "opt_n = optim.SGD(net.parameters(), lr=lr)\n",
        "\n",
        "# Training stage\n",
        "from tqdm import tqdm\n",
        "for epoch in range(1, num_epochs+1):\n",
        "    train_loader_iter = iter(train_loader)\n",
        "    train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
        "    \n",
        "    for _, (imgs, labels) in tqdm(enumerate(train_loader_iter)):\n",
        "        net.train()\n",
        "        opt_n.zero_grad()\n",
        "        if torch.cuda.is_available():\n",
        "            imgs = imgs.cuda()\n",
        "            labels = labels.cuda()\n",
        "        # Label prediction from LeNet\n",
        "        y_hat = net(imgs)\n",
        "        l = loss(y_hat, labels)\n",
        "        # Backprobagation\n",
        "        l.backward()\n",
        "        opt_n.step()\n",
        "\n",
        "        # Calculate tarining error\n",
        "        with torch.no_grad():\n",
        "            labels = labels.long()\n",
        "            train_l_sum += l.item()\n",
        "            train_acc_sum += (torch.sum(torch.argmax(y_hat, dim=1) == labels)).float().item()\n",
        "            n += labels.shape[0]\n",
        "    # calculate testing error every epoch.\n",
        "    test_acc = evaluate_accuracy(iter(test_loader), net)\n",
        "    print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\n",
        "          % (epoch, train_l_sum/n, train_acc_sum/n, test_acc,\n",
        "            time.time() - start))\n",
        "\n",
        "\n",
        "            \n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training using GPU.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "469it [01:16,  6.11it/s]\n",
            "1it [00:00,  6.08it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 1, loss 0.0134, train acc 0.358, test acc 0.503, time 81.1 sec\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "469it [01:17,  6.04it/s]\n",
            "1it [00:00,  6.03it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 2, loss 0.0066, train acc 0.689, test acc 0.758, time 81.9 sec\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "469it [01:18,  5.97it/s]\n",
            "1it [00:00,  5.77it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 3, loss 0.0048, train acc 0.775, test acc 0.767, time 82.8 sec\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "469it [01:18,  5.95it/s]\n",
            "1it [00:00,  5.94it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 4, loss 0.0041, train acc 0.808, test acc 0.745, time 83.1 sec\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "469it [01:19,  5.93it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 5, loss 0.0037, train acc 0.821, test acc 0.830, time 83.3 sec\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}