{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "L6/4 Multilayer Perceptron in PyTorch",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XxG4w0uwrzJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "from torchvision import transforms\n",
        "import torch.optim as optim\n",
        "import time\n",
        "\n",
        "batch_size = 256\n",
        "transform = transforms.Compose([transforms.ToTensor(),]) \n",
        "\n",
        "mnist_train = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "mnist_test = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size,\n",
        "    shuffle=True, num_workers=0)\n",
        "test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size,\n",
        "    shuffle=True, num_workers=0)\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xa9Qiqg3w3Vh",
        "colab_type": "text"
      },
      "source": [
        "# **The Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6icDqql8w2qD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "8685aaaa-37d4-4eb4-a524-db8f248c8114"
      },
      "source": [
        "class Flatten(torch.nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x.view(-1,784)\n",
        "\n",
        "net = nn.Sequential(Flatten(),\n",
        "                    nn.Linear(784,256),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(256,10),\n",
        "                    )\n",
        "\n",
        "def init_weights(m):\n",
        "    if type(m) == nn.Linear:\n",
        "        # Initialize weight parameter by a normal distribition \n",
        "        # with a mean of 0 and standard deviation of 0.01.\n",
        "        nn.init.normal_(m.weight.data, std=0.01)\n",
        "        # The bias parameter is initialized to zero by default.\n",
        "        m.bias.data.fill_(0.0)\n",
        "\n",
        "net.apply(init_weights)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Flatten()\n",
              "  (1): Linear(in_features=784, out_features=256, bias=True)\n",
              "  (2): ReLU()\n",
              "  (3): Linear(in_features=256, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqyVBwT0xPDm",
        "colab_type": "text"
      },
      "source": [
        "# **Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AdJMGKJxOZW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "0ab5c454-bbb2-4a46-86f1-7f392a2a8951"
      },
      "source": [
        "loss = nn.CrossEntropyLoss()\n",
        "lr, num_epochs = 0.5, 5\n",
        "opt_n = optim.SGD(net.parameters(), lr=lr)\n",
        "\n",
        "def evaluate_accuracy(data_iter, net):\n",
        "    \"\"\"Evaluate accuracy of a model on the given data set.\"\"\"\n",
        "    acc_sum,n = 0,0\n",
        "    for (imgs, labels) in data_iter:\n",
        "        net.eval()\n",
        "        with torch.no_grad():\n",
        "            labels = labels.float()\n",
        "            acc_sum += torch.sum((torch.argmax(net(imgs), dim=1) == labels)).float()\n",
        "            n += labels.shape[0]\n",
        "    return acc_sum.item()/n\n",
        "\n",
        "for epoch in range(1, num_epochs+1):\n",
        "    train_loader_iter = iter(train_loader)\n",
        "    train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
        "    from tqdm import tqdm\n",
        "    for _, (imgs, labels) in tqdm(enumerate(train_loader_iter)):\n",
        "        net.train()\n",
        "        opt_n.zero_grad()\n",
        "        if torch.cuda.is_available():\n",
        "            imgs = imgs.cuda()\n",
        "            labels = labels.cuda()\n",
        "        # Label prediction from LeNet\n",
        "        y_hat = net(imgs)\n",
        "        l = loss(y_hat, labels)\n",
        "        # Backprobagation\n",
        "        l.backward()\n",
        "        opt_n.step()\n",
        "\n",
        "        # Calculate tarining error\n",
        "        with torch.no_grad():\n",
        "            labels = labels.long()\n",
        "            train_l_sum += l.item()\n",
        "            train_acc_sum += (torch.sum(torch.argmax(y_hat, dim=1) == labels)).float().item()\n",
        "            n += labels.shape[0]\n",
        "    # calculate testing error every epoch.\n",
        "    test_acc = evaluate_accuracy(iter(test_loader), net)\n",
        "    print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\n",
        "          % (epoch, train_l_sum/n, train_acc_sum/n, test_acc,\n",
        "            time.time() - start))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "235it [00:05, 40.19it/s]\n",
            "4it [00:00, 39.09it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 1, loss 0.0032, train acc 0.693, test acc 0.710, time 6.8 sec\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "235it [00:06, 37.39it/s]\n",
            "4it [00:00, 37.77it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 2, loss 0.0020, train acc 0.809, test acc 0.829, time 7.1 sec\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "235it [00:05, 43.35it/s]\n",
            "5it [00:00, 44.19it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 3, loss 0.0017, train acc 0.840, test acc 0.824, time 6.2 sec\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "235it [00:05, 42.53it/s]\n",
            "5it [00:00, 42.51it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 4, loss 0.0015, train acc 0.855, test acc 0.830, time 6.3 sec\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "235it [00:05, 43.10it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 5, loss 0.0015, train acc 0.863, test acc 0.860, time 6.2 sec\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
