{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "L3/2 Autograd",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iV4PMpr6K1Z",
        "colab_type": "text"
      },
      "source": [
        "# **Automatic Differentiation**\n",
        "\n",
        "**Import autograd and create a variable and attach gradient**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nla7nDMa6VFP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "dbcf600f-d7eb-4cb9-834e-692a36e28763"
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "\n",
        "x = Variable(torch.arange(4, dtype=torch.float32).reshape((4, 1)), requires_grad=True)\n",
        "print(x)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.],\n",
            "        [1.],\n",
            "        [2.],\n",
            "        [3.]], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ez9ekCPp8ptI",
        "colab_type": "text"
      },
      "source": [
        "Now compute $y=2\\mathbf{x}^T\\mathbf{x}$, by placing code inside a *with torch.enable_grad()* block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GskmJFR6vq0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "18d5f6c7-e7a3-41e8-dcee-c08c2d927dbd"
      },
      "source": [
        "with torch.enable_grad():\n",
        "    y = 2 * torch.matmul(x.T, x)\n",
        "\n",
        "print(y)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[28.]], grad_fn=<MulBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26VL_vbi8-Mt",
        "colab_type": "text"
      },
      "source": [
        "**Backward**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWmJKfTi9Asz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y.backward()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_Qdx1cr9EUz",
        "colab_type": "text"
      },
      "source": [
        "**Get the gradient**\n",
        "\n",
        "Given $y=2\\mathbf{x}^T\\mathbf{x}$, we know $\\frac{\\partial y}{\\partial \\mathbf{x}}=4\\mathbf{x}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P63EF9W89D0_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "8b3ba11e-f9f7-4554-fdd9-50f04f5c6030"
      },
      "source": [
        "# Check if each graient value x.grad is equal to 4*x\n",
        "print((x.grad - 4 * x).norm().item() == 0)\n",
        "print(x.grad)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "tensor([[ 0.],\n",
            "        [ 4.],\n",
            "        [ 8.],\n",
            "        [12.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6YmmgT09wCv",
        "colab_type": "text"
      },
      "source": [
        "**Backward on non-scalar**\n",
        "\n",
        "Unlike MxNet, in Pytoch grad can be implicitly created only for scalar outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88STH8bO-yhJ",
        "colab_type": "text"
      },
      "source": [
        "**Computing the hradient of Python control flow**\n",
        "\n",
        "Autograd also works with Python functions and control flows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DRxjJYm-yBw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def f(a):\n",
        "    b = a * 2\n",
        "    while b.norm().item() < 1000:\n",
        "        b = b * 2\n",
        "    if b.sum().item() > 0:\n",
        "        c = b\n",
        "    else:\n",
        "        c = 100 * b\n",
        "    return c"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoxKr_nt_hoM",
        "colab_type": "text"
      },
      "source": [
        "**Function behaviors depends on inputs**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izhVV2Hd-_0a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = torch.randn((1,))\n",
        "a.requires_grad=True\n",
        "with torch.enable_grad():\n",
        "  d = f(a)\n",
        "d.backward()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xr6--Y7D_jRL",
        "colab_type": "text"
      },
      "source": [
        "**Verify the results**\n",
        "$f$ is piecewise linear in its input $a$. There exists $g$ such as $f(a) = ga$ and $\\frac{\\partial f}{\\partial a}=g$. Verify the result:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkNRZKYD_aWK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "72d36952-8f06-4a07-e05d-d3bc3ff1179a"
      },
      "source": [
        "print(a.grad == (d / a))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([True])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdenRdyC_2Z8",
        "colab_type": "text"
      },
      "source": [
        "**Head gradients and the chain rule**\n",
        "\n",
        "We can break the chain rule manually. Assume $\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial y} \\frac{\\partial y}{\\partial x}$. *y.backward()* will only compute $\\frac{\\partial y}{\\partial x}$. To get $\\frac{\\partial z}{\\partial x}$, we can first compute $\\frac{\\partial z}{\\partial y}$, and then pass it as head gradient to *y.backward*."
      ]
    }
  ]
}