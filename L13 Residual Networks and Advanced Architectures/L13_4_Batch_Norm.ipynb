{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "L13/4 Batch Norm",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3gOo80ptyQR"
      },
      "source": [
        "# **Batch Normalization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHknCevnsJA-"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import torchvision.datasets as datasets\n",
        "from torchvision import transforms\n",
        "\n",
        "def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):\n",
        "    # Use is_grad_enabled to determine whether the current mode is training mode\n",
        "    # or prediction mode.\n",
        "    if not torch.is_grad_enabled():\n",
        "        # use the moving average\n",
        "        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)\n",
        "    else:\n",
        "        assert len(X.shape) in (2, 4)\n",
        "        if len(X.shape) == 2: # fully connected layer\n",
        "            mean = X.mean(axis=0)\n",
        "            var = ((X - mean) ** 2).mean(axis=0)\n",
        "        else: # convolution, hence per layer\n",
        "            mean = X.mean(axis=(0, 2, 3), keepdims=True)\n",
        "            var = ((X - mean) ** 2).mean(axis=(0, 2, 3), keepdims=True)\n",
        "        # In training mode, the current mean and variance are used for the \n",
        "        #standardization.\n",
        "        X_hat = (X - mean) / torch.sqrt(var + eps)\n",
        "        # Update the mean and variance of the moving average.\n",
        "        moving_mean = momentum * moving_mean + (1.0 - momentum) * mean\n",
        "        moving_var = momentum * moving_var + (1.0 - momentum) * var\n",
        "    Y = gamma * X_hat + beta # Scale and shift.\n",
        "    return Y, moving_mean, moving_var"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0n1yzTJts8Z"
      },
      "source": [
        "## **BatchNorm Layer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XScKgvZUtuYp"
      },
      "source": [
        "class BatchNorm(nn.Module):\n",
        "    def __init__(self, num_features, num_dims):\n",
        "        super(BatchNorm, self).__init__()\n",
        "        if num_dims == 2:\n",
        "            shape = (1, num_features)\n",
        "        else:\n",
        "            shape = (1, num_features, 1, 1)\n",
        "        # The scale parameter and the shift parameter involved in gradient finding\n",
        "        # and iteration are initialized to 0 and 1 respectively.\n",
        "        self.gamma = nn.Parameter(torch.ones(shape))\n",
        "        self.beta = nn.Parameter(torch.zeros(shape))\n",
        "        # All the variables not involved in gradient finding and iteration are \n",
        "        # initialized to 0 on the CPU.\n",
        "        self.moving_mean = torch.zeros(shape)\n",
        "        self.moving_var = torch.zeros(shape)\n",
        "    def forward(self, X):\n",
        "        # If X is not on the CPU, copy moving_mean and moving_var to the device \n",
        "        # where X is located.\n",
        "        if self.moving_mean.device != X.device:\n",
        "            self.moving_mean = self.moving_mean.to(X.device)\n",
        "            self.moving_var = self.moving_var.to(X.device)\n",
        "        # Save the updated moving_mean and moving_var.\n",
        "        Y, self.moving_mean, self.moving_var = batch_norm(\n",
        "            X, self.gamma, self.beta, self.moving_mean, \n",
        "            self.moving_var, eps=1e-5, momentum=0.9)\n",
        "        return Y"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJV0fxOPuvAW"
      },
      "source": [
        "## **LeNet with Batch Norm**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFV89NUYunX8"
      },
      "source": [
        "class flatten(nn.Module):\n",
        "    def forward(self, input):\n",
        "        return input.view(input.size(0), -1)\n",
        "\n",
        "net = nn.Sequential(nn.Conv2d(1, 6, kernel_size=5),\n",
        "        BatchNorm(6, num_dims=4),\n",
        "        nn.Sigmoid(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        nn.Conv2d(6, 16, kernel_size=5),\n",
        "        BatchNorm(16, num_dims=4),\n",
        "        nn.Sigmoid(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        flatten(),\n",
        "        nn.Linear(256, 120),\n",
        "        BatchNorm(120, num_dims=2),\n",
        "        nn.Sigmoid(),\n",
        "        nn.Linear(120, 84),\n",
        "        BatchNorm(84, num_dims=2),\n",
        "        nn.Sigmoid(),\n",
        "        nn.Linear(84, 10))\n",
        "\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        m.bias.data.normal_(0.0, 0.01)\n",
        "    elif classname.find('Linear') != -1:\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        m.bias.data.normal_(0.0, 0.01)\n",
        "\n",
        "\n",
        "def evaluate_accuracy(data_iter, net):\n",
        "    \"\"\"Evaluate accuracy of a model on the given data set.\"\"\"\n",
        "    acc_sum,n = 0,0\n",
        "    for (imgs, labels) in data_iter:\n",
        "        # send data to the GPU if cuda is availabel\n",
        "        if torch.cuda.is_available():\n",
        "            imgs = imgs.cuda()\n",
        "            labels = labels.cuda()\n",
        "        net.eval()\n",
        "        with torch.no_grad():\n",
        "            labels = labels.long()\n",
        "            acc_sum += torch.sum((torch.argmax(net(imgs), dim=1) == labels)).float()\n",
        "            n += labels.shape[0]\n",
        "    return acc_sum.item()/n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUHQGQ0JxbGZ",
        "outputId": "ad7da779-82aa-4dbe-c485-c4ce6cd222f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    print('Training using GPU.')\n",
        "    net.cuda()\n",
        "else:\n",
        "    print('Training using CPU.')\n",
        "\n",
        "#Initialize network parameters.\n",
        "net.apply(weights_init)\n",
        "\n",
        "lr, num_epochs, batch_size = 1.0, 5, 256\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor()]) \n",
        "mnist_trainset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "mnist_testset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
        "# Loading training set and test set using DataLoader.\n",
        "train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=batch_size,\n",
        "    shuffle=True, num_workers=0)\n",
        "test_loader = torch.utils.data.DataLoader(mnist_testset, batch_size=batch_size,\n",
        "    shuffle=True, num_workers=0)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    net.train() # Switch to training mode\n",
        "    n, start = 0, time.time()\n",
        "    train_l_sum = torch.tensor([0.0], dtype=torch.float32)\n",
        "    train_acc_sum = torch.tensor([0.0], dtype=torch.float32)\n",
        "    train_iter = iter(train_loader)\n",
        "    for X, y in train_iter:\n",
        "        optimizer.zero_grad()\n",
        "        if torch.cuda.is_available():\n",
        "            X = X.cuda()\n",
        "            y = y.cuda()\n",
        "            train_l_sum = train_l_sum.cuda()\n",
        "            train_acc_sum = train_acc_sum.cuda()\n",
        "        y_hat = net(X)\n",
        "        loss = criterion(y_hat, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        with torch.no_grad():\n",
        "            y = y.long()\n",
        "            train_l_sum += loss.float()\n",
        "            train_acc_sum += (torch.sum((torch.argmax(y_hat, dim=1) == y))).float()\n",
        "            n += y.shape[0]\n",
        "\n",
        "    test_acc = evaluate_accuracy(iter(test_loader), net) \n",
        "    print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\\\n",
        "        % (epoch + 1, train_l_sum/n, train_acc_sum/n, test_acc, time.time() - start))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training using GPU.\n",
            "epoch 1, loss 0.0025, train acc 0.766, test acc 0.821, time 5.3 sec\n",
            "epoch 2, loss 0.0015, train acc 0.858, test acc 0.797, time 5.3 sec\n",
            "epoch 3, loss 0.0013, train acc 0.877, test acc 0.734, time 5.1 sec\n",
            "epoch 4, loss 0.0012, train acc 0.886, test acc 0.784, time 5.3 sec\n",
            "epoch 5, loss 0.0012, train acc 0.893, test acc 0.805, time 5.2 sec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQTS8Klbz6zk"
      },
      "source": [
        "# **BatchNorm with PyTorch Built-in Function**\n",
        "\n",
        "The performance of two types of implementation are quite similar, while the PyToch built-in function is around 10% faster. This is due to the PyTorch build-in function was written and compiled in C++."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmQHdBNVz6aA",
        "outputId": "58a746ad-2567-4424-c20e-d68a7e1249db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "net = nn.Sequential(\n",
        "        nn.Conv2d(1, 6, kernel_size=5),\n",
        "        nn.BatchNorm2d(6),\n",
        "        nn.Sigmoid(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        nn.Conv2d(6, 16, kernel_size=5),\n",
        "        nn.BatchNorm2d(16),\n",
        "        nn.Sigmoid(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        flatten(),\n",
        "        nn.Linear(256, 120),\n",
        "        nn.BatchNorm1d(120),\n",
        "        nn.Sigmoid(),\n",
        "        nn.Linear(120, 84),\n",
        "        nn.BatchNorm1d(84),\n",
        "        nn.Sigmoid(),\n",
        "        nn.Linear(84, 10))\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print('Training using GPU.')\n",
        "    net.cuda()\n",
        "else:\n",
        "    print('Training using CPU.')\n",
        "\n",
        "#Initialize network parameters.\n",
        "net.apply(weights_init)\n",
        "\n",
        "lr, num_epochs, batch_size = 1.0, 5, 256\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor()]) \n",
        "mnist_trainset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "mnist_testset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
        "# Loading training set and test set using DataLoader.\n",
        "train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=batch_size,\n",
        "    shuffle=True, num_workers=0)\n",
        "test_loader = torch.utils.data.DataLoader(mnist_testset, batch_size=batch_size,\n",
        "    shuffle=True, num_workers=0)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    net.train() # Switch to training mode\n",
        "    n, start = 0, time.time()\n",
        "    train_l_sum = torch.tensor([0.0], dtype=torch.float32)\n",
        "    train_acc_sum = torch.tensor([0.0], dtype=torch.float32)\n",
        "    train_iter = iter(train_loader)\n",
        "    for X, y in train_iter:\n",
        "        optimizer.zero_grad()\n",
        "        if torch.cuda.is_available():\n",
        "            X = X.cuda()\n",
        "            y = y.cuda()\n",
        "            train_l_sum = train_l_sum.cuda()\n",
        "            train_acc_sum = train_acc_sum.cuda()\n",
        "        y_hat = net(X)\n",
        "        loss = criterion(y_hat, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        with torch.no_grad():\n",
        "            y = y.long()\n",
        "            train_l_sum += loss.float()\n",
        "            train_acc_sum += (torch.sum((torch.argmax(y_hat, dim=1) == y))).float()\n",
        "            n += y.shape[0]\n",
        "\n",
        "    test_acc = evaluate_accuracy(iter(test_loader), net) \n",
        "    print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\\\n",
        "        % (epoch + 1, train_l_sum/n, train_acc_sum/n, test_acc, time.time() - start))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training using GPU.\n",
            "epoch 1, loss 0.0025, train acc 0.771, test acc 0.476, time 4.7 sec\n",
            "epoch 2, loss 0.0015, train acc 0.860, test acc 0.622, time 4.7 sec\n",
            "epoch 3, loss 0.0014, train acc 0.874, test acc 0.766, time 4.7 sec\n",
            "epoch 4, loss 0.0013, train acc 0.884, test acc 0.734, time 4.6 sec\n",
            "epoch 5, loss 0.0012, train acc 0.891, test acc 0.824, time 4.7 sec\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
